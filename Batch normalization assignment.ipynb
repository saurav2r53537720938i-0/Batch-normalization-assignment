{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9760444e-219e-4ce7-a761-09d4a1ea4b64",
   "metadata": {},
   "source": [
    "Batch normalization is a technique used in neural networks to improve traning speed and anylyze it works by normalization war to develop by anyone typically doing the during traning this the problembs irritate shift which can slow down traning by causing the distributio of inputs to change as the network parametrs are updated addtionally batch normalization acts as a regularizer reducing the need for other regularization the need for other regularization regulaizer technique like dropouts overall it improves the convergence of the network during traning and allows for the faster during traning and allows for faster traning times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d0883-74dc-4226-85fd-903f9b910af2",
   "metadata": {},
   "source": [
    "Using batch normalization during traning offers sevral benfits:\n",
    "1. Improved Traaning Speed\n",
    "2. Stablity in Traning\n",
    "3. Better Gradient flow\n",
    "4. Regularization\n",
    "5. Improved Genralization\n",
    "Overall batch normalization is a powerful technique that improves the stablity speed and genralization perfomance of neural networks during traning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb56bf-b2b0-4cc4-87df-3082974f6263",
   "metadata": {},
   "source": [
    "Batch normalization works by normalizing the inputs of each layer in a neural network here how it typically operates:\n",
    "1. Normalization steps\n",
    "2. Scaling and Shifting\n",
    "3. Learnable Parmetrs\n",
    "4. Inflence\n",
    "By normalizing the inputs of each layer and allowing the network to learn the optimal scaling and shifting traning improve covergence and enhance the genralization performance of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a920464-74ee-4330-9be1-3114a58f2987",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1966619928.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    from tenseorflow.Keras import\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tenseflow as tf\n",
    "from tenseorflow.Keras import\n",
    "datsets\n",
    "from sklearn.model_slection import\n",
    "train_test_split\n",
    "from sklearn.preprocessing import\n",
    "standardScaler\n",
    "\n",
    "# Load CIFAR-10 datsets\n",
    "(x_train,y_train), (x_test,y_test)=\n",
    "datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea175ad-d8f6-46a7-9eea-fb66e4007aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
